{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classical MDS\n",
    "<!-- 11-1 -->"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Classical Multi Dimensional Scaling: introduction***\n",
    "\n",
    "MDS의 아이디어는 데이터간 거리를 보존하는 map을 학습하는 것임을 살펴보았습니다. 고전적인 MDS는 차원축소를 이용하여 거리를 보존하는 방법을 제안하였습니다. 차원축소를 이용한다는 것은 거리행렬 $D^2$과 $\\hat D^2$의 차이를 operator norm (일종의 행렬의 길이를 재는 방법)으로 정의하고 적절한 $\\hat D^2$를 찾는 방법입니다.\n",
    "\n",
    "이제 $\\mathbb{R}^d$위에 있는 원본 데이터 $x_i$와 $x_j$의 거리 $d_{ij}$를 데이터로 표현하는 방법을 살펴보겠습니다. \n",
    "\n",
    "$$d_{ij}^2 = \\|x_i-x_j\\|^2 = x_i^\\top x_i + x_j^\\top x_j -2x_i^\\top x_j$$\n",
    "\n",
    "여기서 데이터 행렬 $X \\in \\mathbb{R}^{n\\times d}$에 대해 \n",
    "$$B = XX^\\top\\in \\mathbb{R}^{n\\times n}$$\n",
    "정의하겠습니다. 행렬 $B$의 $i$행 $j$열 원소를 $b_{ij}$라 한다면 $d_{ij}$와 $b_{ij}$는 다음 관계를 만족합니다.\n",
    "\n",
    "$$d_{ij}^2 = b_{ii} + b_{jj} - 2b_{ij}$$\n",
    "왜냐하면 $b_{ij} = x_i^\\top x_j$이 기 때문이지요. \n",
    "\n",
    "여기서 우리는 데이터로부터 계산할 수 있는 $d_{ij}^2$로 부터 $b_{ij}$의 값을 계산할 수 있습니다. 정확하기 말하면 임의의 $b_{ij}$를 $d_{ij}$의 식으로 표현할 수 있습니다.\n",
    "\n",
    "먼저, $T=tr(B)$라 놓습니다. 구체적으로 $T=\\sum_{i=1}^n b_{ii}=\\sum_{i=1}^n \\|x_i\\|^2$로 계산할 수 있습니다. 다음 $D^2$ 행렬의 원소값은 위 $$d_{ij}^2 = b_{ii} + b_{jj} - 2b_{ij}$$의 관계식과 $\\sum_{j=1}^n x_{ij} = 0$ for each $i$의 식을 이용하면 아래 결과들을 얻을 수 있습니다.  여기서 $\\sum_{j=1}^n x_{ij} = 0$ centering 가정은 data 가 표준화되었다는 것으로 부터 나온 것이며, $\\sum_{j=1}^n b_{ij}=\\sum_{j=1}^n x_i^\\top x_j = x_i^\\top (\\sum_{j=1}^n x_j) = x_i^\\top {\\bf 0}  = 0$입니다. \n",
    "- $\\sum_{i,j}d_{ij}^2= 2nT$\n",
    "- $\\sum_{i=1}^n d_{ij}^2 =T + nb_{jj}$ for each $i$\n",
    "- $\\sum_{j=1}^n d_{ij}^2 =T + nb_{ii}$ for each $j$\n",
    "\n",
    "$D^2$행렬은 주어져 있으므로 다음과 같이 $b_{ij}$값을 구할 수 있습니다.\n",
    "- $T = \\sum_{i=1}^n \\|x_i\\|^2$를  계산합니다.\n",
    "- 이미 구한 $T$값과 $\\sum_{i=1}^n d_{ij}^2 =T + nb_{jj}$의 관계를 이용하여 $b_{jj}$ for all $j$를 구합니다. \n",
    "- 이미 구한 $b_{jj}$ for all $j$와 $d_{ij}^2 = b_{ii} + b_{jj} - 2b_{ij}$ 관계를 이용하여 $b_{ij}$를 구합니다.\n",
    "\n",
    "$$b_{ij} = -1/2(d_{ij}^2 - d_{i\\cdot}^2 - d_{\\cdot j}^2 + d_{\\cdot \\cdot}^2)$$\n",
    "(단, $d_{i\\cdot}^2 = n^{-1}\\sum_{j = 1}^n d_{ij}^2$ 그리고 $d_{\\cdot j}^2 = n^{-1}\\sum_{i = 1}^n d_{ij}^2$)\n",
    "\n",
    "\n",
    "***Implication 1***\n",
    "\n",
    "위 과정을 통해 우리는 다음 두 가지 결론을 내릴 수 있습니다. \n",
    "- 내적행렬 $B$가 주어진 경우에 거리행렬 $D^2$를 구할 수 있습니다. \n",
    "- 데이터간의 거리행렬($D^2$)이 주어진 경우 내적행렬 $B:=XX^\\top$을 구할 수 있습니다. \n",
    "- 다시 말해 $B$와 $D^2$은 1-1의 관계가 있습니다. 특별히 \n",
    "$$b_{ij} = -1/2( d_{ij}^2 - d_{i\\cdot}^2 - d_{\\cdot j}^2 + d_{\\cdot \\cdot}^2)$$\n",
    "의 관계는  $b_{ij}$값은 $D^2$ 행렬을 행방향 그리고 열방향으로 표준화된 값임을 알 수 있습니다.\n",
    "이를 행렬으로 표현을 하면\n",
    "$$B = -\\frac{1}{2}(I-\\Pi_{1})D^2(I-\\Pi_{1})$$\n",
    "입니다. 단 $\\Pi_{1}$은 $1\\in \\mathbb{R}^d$ 열벡터공간의 정사영행렬로 $\\Pi_{1} = 1(1^\\top 1)^{-1}1^\\top$로 주어집니다. \n",
    "\n",
    "***Classical Multi Dimensional Scaling***\n",
    "우리의 논의를 이제 원래 데이터 공간이 아닌 축약된 공간으로 확장해봅시다. 축약된 공간의 데이터 $\\hat x_i \\in \\mathbb{R}^q$가 있고 거기서 $\\hat D^2 \\in \\mathbb{n\\times n}$ 행렬이 있다고 합시다. 단 지금 $\\hat D^2$의 구체적인 원소값을 알 필요는 없습니다. 그리고 $\\hat x_i$의 내적행렬 $\\hat B \\mathbb{n\\times n}$가 있다고 합시다. 이 때 $\\hat D^2$와 $\\hat B$는 어떤 관계를 가져야 할까요?\n",
    "\n",
    "$$\\hat B = -\\frac{1}{2}(I-\\Pi_{1})\\hat D^2(I-\\Pi_{1})$$ \n",
    "일 것입니다. 왜냐하면 우리가 위의 식을 유도하는데 거리는 유클리디안 거리라는 가정만 사용하였기 때문이지요. 한편\n",
    "MDS의 원래 idea를 다시 상기해봅시다. \n",
    "\n",
    "축소된 공간에서 거리를 원래 공간의 거리가 되도록 최대한 보존하는 차원 축양방법이 무엇일까?\n",
    "\n",
    "그러면 목적함수는 Frobenius norm (제곱)을 활용하여 계산하면 되겠지요?\n",
    "\n",
    "$$\\min \\|\\hat D^2 -  D^2\\|_F^2$$\n",
    "\n",
    "그리고 Frobenius norm의 특징을 이용하면 다음과 같은 결과를 얻을 수 있습니다. \n",
    "\\begin{aligned}\n",
    "\\|\\hat D^2 -  D^2\\|_F^2 &=& \\|  (I-\\Pi_{1})(\\hat D^2 -  D^2) \\|_F^2 +   \\|  \\Pi_{1}(\\hat D^2 -  D^2) \\|_F^2\\\\\n",
    "&=& \\|  (I-\\Pi_{1})(\\hat D^2 -  D^2)(I-\\Pi_{1}) \\|_F^2 +  \\|  (I-\\Pi_{1})(\\hat D^2 -  D^2)\\Pi_1 \\|_F^2\\\\\n",
    "&&+ \\|  \\Pi_{1}(\\hat D^2 -  D^2)(I-\\Pi_{1}) \\|_F^2 +  \\|  \\Pi_{1}(\\hat D^2 -  D^2)\\Pi_1 \\|_F^2\n",
    "\\end{aligned}\n",
    "\n",
    "이 식으로 부터 만약 우리가 새로 만들어진 $\\hat D^2$에 대해 $\\hat d_{ij}^2$의 열의 합, 행의 합을 일정하게 유지한다는 가정을 한다면 $\\|\\hat D^2 -  D^2\\|_F^2$의 값이 실제로는 $\\|  (I-\\Pi_{1})(\\hat D^2 -  D^2)(I-\\Pi_{1}) \\|_F^2$에만 의존함을 알 수 있습니다. 흥미롭게도 이 식은 $\\|  \\hat B - B \\|_F^2$와 같으며 <u>우리는 MDS 목적이 저차원에서 정의된 새로운 데이터들의 내적행렬이 원래 공간에서 내적행렬에 근사하도록 만드는 문제라는 것을 알 수 있습니다. </u>\n",
    "\n",
    "따라서 $\\hat B$는 $B$행렬에 대한 고유치분해 (eigen decomposition)을 통해 값을 계산하게 됩니다. \n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 방법은 데이터의 missing 있는 경우에 작동하지 않습니다. 그리고 계산량이 $O(n^3)$으로 알려져 있어 이를 개선하기 위한 방법론들이 제안되었습니다. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "09d430ac884b7b963decef9a3390e18440a81a639eb9d9528489333ac105a61d"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multidimensional Scaling \n",
    "<!-- 11-2 -->\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Why we use the Multidimensional Scaling***\n",
    "\n",
    "온라인 상점에서 고객의 정보를 처리하는 부서에서 고차원의 데이터의 데이터를 분석하는 상황을 생각해봅시다. 고객 한 명은 인구통계학적 정보와 구매 기록정보, 온라인 상점의 서비스센터와의 상담정보등 수많은 데이터를 가지고 있습니다. 이는 고객 한 명을 표현하는데 이러한 수많은 정보들이 사용되고 고객의 한 명의 표현에 이 정보가 사용된다는 뜻입니다. 데이터 수집 비용이 감소하고 더 많은 데이터를 축적하고 관리할 수 있는 기회가 늘어나면서 많은 회사들은 한 고객에 대한 더 풍부한 표현형에 대해 고민하게 되었습니다. \n",
    "\n",
    "이렇게 고차원의 정보로 고객을 표현할 수 있게 되면서 다양한 분석을 시도할 수 있게 되었지만, 그 고객들을 비슷한 사람들끼리 묶거나 분류하기 위한 작업은 점점 어려워집니다. 만약 한 고객이 두 가지 종류의 정보로 표현할 수 있다면 우리는 2차원 평면상의 시각화된 산점도로서 고객 집단의 패턴을 식별해낼 수 있을 것입니다. 고차원 데이터로 표현된 고객 정보는 2차원 상에서 구현된 시각적인 도구를 활용하여 파악하기가 어려울 것입니다. 특히 고객간의 유사성에 대한 전체적인 패턴을 파악하기는 거의 불가능 할 것입니다. 이는 우리가 가진 인지적 한계로 부터 오는 문제이겠지요. \n",
    "\n",
    "Multidimensional Scaling(MDS)은 이러한 문제의식으로 부터 출발합니다. MDS가 개발된 본질적인 물음은 다음과 같습니다. \n",
    "<center>\n",
    "고차원에서 표현된 데이터들을 저차원에서 어떻게 효과적으로 볼 수 있을까?\n",
    "</center>\n",
    "\n",
    "이 물음에 답하기 위해서 MDS는 다음과 같은 방법으로 구현된 방법론입니다. \n",
    "- 고차원에서 각 데이터 쌍들의 거리를 저차원에서도 최대한 유지시킬 수 있는 저차원 표현형이 무엇일까?\n",
    "- 저차원 표현형을 잘 계산하기 위해서 어떠한 방법을 사용하면 될까?\n",
    "\n",
    "먼저 MDS를 이해하기 위해서 거리에 대한 개념부터 확인해보겠습니다.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Idea of Multi Dimensional Scaling***\n",
    "\n",
    "두 원소 $x, y$가 있다고 합시다. 두 원소 $x,y$의 거리를 $d(x,y)$라고 합시다.\n",
    "\n",
    "$d(x,y)$를 함수로 보면 거리함수는 두 원소를 입력으로 받아서 실수값을 출력으로 합니다. 이를 좀 더 자세히 표현해볼까요?\n",
    "\n",
    "- $\\mathcal{X}$: 데이터의 입력공간\n",
    "- $d:\\mathcal{X} \\times \\mathcal{X} \\mapsto \\mathbb{R}$: 거리함수, 두 데이터를 받아서 실수 값을 출력으로 함.\n",
    "\n",
    "여기서 $d(\\cdot, \\cdot)$가 거리함수라면 어떤 함수도 거리함수가 될까요? 거리함수는 아래의 성질을 만족해야 합니다.\n",
    "\n",
    "- $d(x,y) \\geq 0$ \n",
    "- $d(x,x) = 0$ \n",
    "- $d(x,y) = d(y,x)$ (대칭성)\n",
    "- $d(x,z) \\leq d(x,y) + d(y,z)$  (삼각부등식)\n",
    "\n",
    "함수 $d$가 위 조건을 만족한다면 모두 거리함수가 됩니다. 그래서 거리는 종류가 매우 많습니다. \n",
    "\n",
    "우리는 고전적인 거리함수를 실생활에서 사용합니다. 바로 유클리디안 거리입니다. 우리가 사용할 유클리디안 거리는 다음과 같습니다.\n",
    "- $\\mathcal{X}$를 $\\mathbb{R}^d$로 놓습니다.\n",
    "- $x = (x_1, \\cdots, x_d), y= (y_1, \\cdots, y_d)$로 $\\mathbb{R}^d$의 원소로 놓습니다.\n",
    "- 유클리디안 거리\n",
    "$$d(x,y) = \\sqrt{\\sum_{j=1}^d (x_i - y_j)^2} $$\n",
    "\n",
    "이제 $d$차원 데이터 $n$개를 생각해봅시다. 각각의 데이터를 $x_1, \\cdots, x_n \\in \\mathbb{R}^d$라고 합시다. 여기서 $x_i$를 아래와 같이 표시합니다.\n",
    "- $x_i = (x_{i1}, \\cdots, x_{id})$\n",
    "\n",
    "여기서 두 데이터 $x_i$와 $x_j$에 대해서 거리를 $d_{ij}$라고 합시다. $d_{ij}^2$은 아래와 같이 쓸 수 있습니다.\n",
    "$$d_{ij}^2 = \\sum_{k=1}^d(x_{ik}-x_{jk})^2$$\n",
    "\n",
    "우리가 데이터들의 거리에만 관심이 있다면 데이터들의 중심에 무관하게 거리의 값이 계산됩니다. 예를 들어 모든 데이터의 중심을 $c=(c_1, \\cdots, c_d)$로 옮기더라고 새로운 데이터 $x_i+c$와 $x_j+c$의 거리는 동일합니다. 그래서 우리가 데이터들의 거리를 구할때는 평균이 0이 되도록 표준화를 하고 구해도 됩니다. (scaling을 하지 않습니다) 그래서 다음과 같이 가정을 해도 거리만들 생각하는데는 문제가 없겠지요.\n",
    "\n",
    "- 가정: $\\sum_{i=1}^n x_i = 0 \\in \\mathbb{R}^d$\n",
    "\n",
    "데이터들의 거리를 계산할 수 있으니 데이터들의 거리를 확인하기 위해서 시각화를 할 수 있을까요? 만약 고차원 자료라면 데이터간의 거리를 확인하기 위한 시각화 결과물을 그릴 수 없을 것입니다. 여기서 우리는 차원 축소를 생각합니다. \n",
    "$d$차원 데이터를 $q$차원 데이터로 축약하는 선형변환은 무엇일까요? \n",
    "\n",
    "$$A:\\mathbb{R}^d \\mapsto \\mathbb{R}^q$$\n",
    "\n",
    "$q\\times d$ 행렬입니다. 데이터 $x$를 $q$차원으로 축약한 데이터는 \n",
    "$$\\hat x = Ax$$\n",
    "가 되겠지요? 그러면 어떤 차원축소 방법 다시 말해 어떤 $A$를 선택해야 할까요? 우리는 애초에 데이터간의 거리를 확인하기 위해서 차원 축소를 하였습니다. 따라서 원래 축약되기 이전에서의 데이터의 거리가 축약된 이후에도 잘 보존되기를 원할 것입니다. 즉 고차원에서 가까운 두 데이터가 저차원 공간에서 여전히 가깝기를 원하겠지요. 우리의 이런 바램을 식으로 표현하면 어떻게 될까요?\n",
    "\n",
    "먼저 축약된 두 데이터 $\\hat x_i:= Ax_i$, $\\hat x_j:= Ax_j$ 를 생각하고 유클리디안 거리를 생각해봅시다.\n",
    "\n",
    "$$\\hat d_{ij} = \\sqrt{\\sum_{k=1}^q (\\hat x_{ik}- \\hat x_{jk})^2}$$\n",
    "\n",
    "그러면 우리는 $\\hat d_{ij} \\simeq \\hat d_{ij}$ 가 되도록 $\\hat d_{ij}$를 만들어야 할 것입니다. 즉, 차원축소 함수인 $A$를 잘 선택해서 $\\hat d_{ij} \\simeq \\hat d_{ij}$이 되도록 만들어야 할 것입니다. 이를 위해 다음 목적함수를 정의하고 아래 식을 최소화 하는  $A$를 찾으면 될 것입니다. \n",
    "\n",
    "$$\\sum_{i,j}\\| \\hat d_{ij} - d_{ij}\\|^2 $$\n",
    "\n",
    "이렇게 $A$를 구하는 방법을 metric Multi Dimensional Scaling (mMDS)라 부릅니다. 여기서 원래 공간에서 distance와 축약된 공간에서 distance의 차이를 제곱거리($l_2$-distance)를 사용하여 목적함수를 정의했지만, 다른 함수를 사용할 수도 있습니다.\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***참고***\n",
    "\n",
    "$d_{ij}^2$ 을 $i$행 $j$열의 원소로 가지는 행렬 $D^2$을 생각해봅시다. 같은 방법으로 $\\hat d^2_{ij}$에 대응하는 $\\hat D^2$을 생각합니다. 행렬이론에서는 행렬에 대한 길이(norm)라는 개념이 있습니다. $D^2-\\hat D^2$의 길이로 행렬 $D^2$과 $\\hat D^2$의 차이를 정의합니다. \n",
    "\n",
    "어떤 정방행렬 $A$의 Frobeneous norm 은 $\\|A\\|_F$이라 표기하고 $\\|A\\|_F = \\sqrt{\\sum_{i,j} a_{ij}^2}$으로 정의합니다.\n",
    " $D^2-\\hat D^2$의 Frobeneous norm 은  $\\|D^2-\\hat D^2\\|_F$라 표기하는데\n",
    " $$ \\|D^2-\\hat D^2\\|_F^2 = \\sum_{i,j} | \\hat d_{ij} - d_{ij}|^2 $$\n",
    "  이 됩니다.\n",
    "\n",
    "Frobeneous norm 의 특성\n",
    "- $\\|A\\|_F^2  = \\sum_{i,j}a_{ij}^2$\n",
    "- $\\|A\\|_F^2  = \\text{Tr}(A^\\top A) = \\sum_{i}\\sigma^2_i$ (단, $\\sigma^2_i$는 singular value of $A$)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tf2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "9a92dc886063db3b8548512bbbf3429dad7ae1708694c583c21d1ffc4fc47ad7"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
